{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbfe8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5f065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8501821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test:\n",
    "    # df = pd.read_pickle(r'/home/jason/Desktop/school/dc/distrf/dataset/cancer.pkl')\n",
    "    df = pd.read_pickle(r\"/Users/duong-jason/Desktop/dc/distrf/dataset/cancer.pkl\")\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db25b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a87a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    df = pd.read_csv(r\"/Users/duong-jason/Desktop/dc/distrf/dataset/golf.csv\")\n",
    "    X_train, y_train = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0a4a2",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation\n",
    "### Entropy *(Bits)*\n",
    "$\\mathcal{H}(t, \\mathcal{D})=-\\sum_{l\\in levels(t)\\\\}^{}{P(t=l)\\cdot\\log_2(P(t=l))}\\ \\text{bits}$\n",
    "\n",
    "### Rem\n",
    "$rem(d,\\mathcal{D})=\\sum_{l\\in levels(t)}{}\\frac{|\\mathcal{D}_{d=l}|}{\\mathcal{D}}\\cdot \\mathcal{H}(t, \\mathcal{D}_{d=l})$\n",
    "\n",
    "### Information Gain\n",
    "$IG(d, \\mathcal{D})=\\mathcal{H}(t, \\mathcal{D})-rem(d, \\mathcal{D})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288856b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        feature=None,\n",
    "        data=None,\n",
    "        branch=None,\n",
    "        parent=None,\n",
    "        leaf=False,\n",
    "        children=[]\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.data = data\n",
    "        self.branch = branch\n",
    "        self.parent = parent\n",
    "        self.leaf = leaf\n",
    "        self.children = children\n",
    "\n",
    "    @property\n",
    "    def isLeaf(self):\n",
    "        return self.leaf\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.data.iloc[:, :-1]\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ed769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"A Rudimentary Decision Tree Classifier\"\"\"\n",
    "    def __init__(self, *, criterion=None):\n",
    "        \"\"\"Pre-pruning criterion = {max_depth, partition_threshold, low_gain}\"\"\"\n",
    "        self.root = None\n",
    "        self.levels = None\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __repr__(self, node=None, depth=0):\n",
    "        \"\"\"Displays the decision tree\"\"\"\n",
    "        if not node:\n",
    "            node = self.root\n",
    "\n",
    "        print(depth * '\\t', node.feature, f\"(Branch={node.branch})\")\n",
    "        for child in node.children:\n",
    "            self.__repr__(child, depth+1)\n",
    "\n",
    "        return \"\"\n",
    "    \n",
    "    def partition(self, X, y, d, t):\n",
    "        \"\"\"Returns a subset of the training data with feature (d) of level (t)\"\"\"\n",
    "        D = pd.concat([X.loc[X[d]==t], y.loc[X[d]==t]], axis=1)\n",
    "        D = D.drop([d], axis=1)\n",
    "        return D.iloc[:, :-1], D.iloc[:, -1], t\n",
    "\n",
    "    def entropy(self, X, y):\n",
    "        \"\"\"Measures the amount of uncertainty/impurity/heterogeneity in (X, y)\"\"\"\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return -sum([proba(t) * np.log2(proba(t)) for t in y.unique()])\n",
    "\n",
    "    def rem(self, X, y, d):\n",
    "        \"\"\"Measures the entropy after partitioning (X, y) on feature (d)\"\"\"\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return sum([weight(t) * self.entropy(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def information_gain(self, X, y, d):\n",
    "        \"\"\"Measures the reduction in the overall entropy in (X, y) achieved by testing on feature (d)\"\"\"\n",
    "        # if debug:\n",
    "        #     print(f\"{d} = {self.entropy(X, y):.3f} - {self.rem(X, y, d):.3f} = {self.entropy(X, y) - self.rem(X, y, d):.3f}\") \n",
    "        return self.entropy(X, y) - self.rem(X, y, d)\n",
    "\n",
    "    def build_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"Performs the ID3 algorithm\"\"\"\n",
    "        if len(y.unique()) == 1:  # all instances have the same target feature values\n",
    "            # if debug:\n",
    "            #     print(\"All instances have the same target feature value\\n\")\n",
    "            return Node(feature=y.iat[0],\n",
    "                        data=pd.concat([X, y], axis=1),\n",
    "                        branch=branch,\n",
    "                        parent=parent,\n",
    "                        leaf=True)\n",
    "        elif X.empty:  # dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "            # if debug:\n",
    "            #     print(\"Dataset is empty\\n\")\n",
    "            return Node(feature=mode(parent.y),\n",
    "                        branch=branch,\n",
    "                        parent=parent,\n",
    "                        leaf=True)\n",
    "        elif all((X[d] == X[d].iloc[0]).all() for d in X.columns):  # if all feature values are identical\n",
    "            # if debug:\n",
    "            #     print(\"All instances have the same descriptive features\\n\")\n",
    "                return Node(feature=mode(y),\n",
    "                            data=pd.concat([X, y], axis=1),\n",
    "                            branch=branch,\n",
    "                            parent=parent,\n",
    "                            leaf=True)\n",
    "        elif self.criterion.get(\"max_depth\"):\n",
    "            if depth >= self.criterion[\"max_depth\"]:\n",
    "                # if debug:\n",
    "                #     print(\"Stopping at Max Depth\\n\")\n",
    "                return Node(feature=mode(y),\n",
    "                            data=pd.concat([X, y], axis=1),\n",
    "                            branch=branch,\n",
    "                            parent=parent,\n",
    "                            leaf=True)\n",
    "        elif self.criterion.get(\"partition_threshold\"):\n",
    "            if len(X) < self.criterion[\"partition_threshold\"]:\n",
    "                # if debug:\n",
    "                #     print(f\"Stopping at {len(X)} instances\\n\")\n",
    "                return Node(feature=mode(y),\n",
    "                            data=pd.concat([X, y], axis=1),\n",
    "                            branch=branch,\n",
    "                            parent=parent,\n",
    "                            leaf=True)\n",
    "\n",
    "        # if debug:\n",
    "        #     print(\"===Information Gain===\")\n",
    "        gain = np.argmax([self.information_gain(X, y, d) for d in X.columns])\n",
    "\n",
    "        if self.criterion.get('low_gain'):\n",
    "            if gain <= self.criterion[\"low_gain\"]:\n",
    "                # if debug:\n",
    "                #     print(f\"Stopping at Gain={gain}\\n\")\n",
    "                return Node(feature=mode(y),\n",
    "                            data=pd.concat([X, y], axis=1),\n",
    "                            branch=branch,\n",
    "                            parent=parent,\n",
    "                            leaf=True)\n",
    "\n",
    "        best_feature = X.columns[gain]\n",
    "        best_node = deepcopy(Node(feature=best_feature,\n",
    "                                  data=pd.concat([X, y], axis=1),\n",
    "                                  branch=branch,\n",
    "                                  parent=parent))\n",
    "\n",
    "        # if debug:\n",
    "        #     print()\n",
    "        #     print(\"===Best Feature===\")\n",
    "        #     print(best_feature)\n",
    "        #     print()\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            # if debug:\n",
    "            #     print(f\"===Partitioned Dataset ({t})===\")\n",
    "            #     print(pd.concat(d, axis=1).head())\n",
    "            #     print()\n",
    "            best_node.children.append(self.build_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.levels = {k: X[k].unique() for k in X.columns}\n",
    "        self.root = self.build_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.isLeaf:\n",
    "            for child in node.children:\n",
    "                if child.branch == x.get(node.feature).values:\n",
    "                    node = child\n",
    "                    break\n",
    "        return node.feature\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = [self.predict(X.iloc[x].to_frame().T) for x in range(len(X))]\n",
    "        # return confusion_matrix(y, y_hat, labels=[1, 0])\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())  # for the golf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96c2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.75 s, sys: 109 ms, total: 6.86 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt = DecisionTree(criterion={'max_depth': 4}).fit(X_train, y_train)\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885a388",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454ac107-8855-4862-8582-e1f55e2f6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=5, n_sample=2, criterion=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_sample = n_sample\n",
    "        self.forest = [DecisionTree(criterion=criterion) for _ in range(n_estimators)]\n",
    "\n",
    "    def sub_sample(self, X, n_sample=2):\n",
    "        \"\"\"Enforces feature randomness\"\"\"\n",
    "        return np.random.choice(X.columns.to_numpy(), n_sample, replace=False)\n",
    "\n",
    "    def bootstrap_sample(self, X, y, n_sample, key=True):\n",
    "        feature_subset = self.sub_sample(X, int(np.log2(len(X))))\n",
    "        d = pd.concat([X, y], axis=1)\n",
    "        d = d.sample(n=n_sample, replace=key)\n",
    "        return d.iloc[:, :-1][feature_subset], d.iloc[:, -1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for tree in self.forest:\n",
    "            tree.fit(*self.bootstrap_sample(X, y, self.n_sample))\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        assert all(isinstance(model, DecisionTree) for model in self.forest)\n",
    "        return mode([dt.predict(x) for dt in self.forest])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = [self.predict(X.iloc[x].to_frame().T) for x in range(len(X))]\n",
    "        # return confusion_matrix(y, y_hat, labels=[1, 0])\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d17d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 1.85 s, total: 21.5 s\n",
      "Wall time: 23.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x2c44373a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForest(n_estimators=10, n_sample=len(X_train), criterion={'partition_threshold': 5}).fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
