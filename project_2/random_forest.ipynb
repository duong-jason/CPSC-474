{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbfe8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from statistics import mode, mean\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8501821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(r\"/Users/duong-jason/Desktop/dc/project_2/dataset/cancer.pkl\")\n",
    "# X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343cf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Stream': ['false', 'true', 'true', 'false', 'false', 'true', 'true'],\n",
    "    'Slope': ['steep', 'moderate', 'steep', 'steep', 'flat', 'steep', 'steep'],\n",
    "    'Elevation': ['high', 'low', 'medium', 'medium', 'high', 'highest', 'high'],\n",
    "    'Vegetation': ['chapparal', 'riparian', 'riparian', 'chapparal', 'conifer', 'conifer', 'chapparal']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74d03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'Season': ['winter', 'winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'autumn', 'autumn', 'autumn'],\n",
    "#     'Work Day': ['false', 'false', 'true', 'false', 'true', 'true', 'false', 'true', 'true', 'false', 'false', 'true'],\n",
    "#     'Rentals': [800, 826, 900, 2100, 4740, 4900, 3000, 5800, 6200, 2910, 2880, 2820]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db25b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0a4a2",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288856b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A Decision Tree Node\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature - the value of a descriptive/target feature of a node\n",
    "    data - the partitioned dataset resulting from the parent node on a feature value\n",
    "    branch - the feature value from the parent node\n",
    "    parent - the immediate adjacent node along the path from the root\n",
    "    leaf - denotes a terminal node whose prediction is based on the path from the root to the node\n",
    "    depth - the number of levels from the root to a node\n",
    "    children - the nodes resulting from each unique feature value of the parent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        feature=None,\n",
    "        data=None,\n",
    "        branch=None,\n",
    "        parent=None,\n",
    "        leaf=False,\n",
    "        depth=0,\n",
    "        children=[]\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.data = data\n",
    "        self.branch = branch\n",
    "        self.parent = parent\n",
    "        self.leaf = leaf\n",
    "        self.depth = depth\n",
    "        self.children = children\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.depth * '\\t' + f\" {self.feature} (Branch={self.branch})\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Returns whether a node is terminal\"\"\"\n",
    "        return self.leaf\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Returns the partitioned feature matrix of a node\"\"\"\n",
    "        return self.data.iloc[:, :-1]\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"Returns the partitioned target vector of a node\"\"\"\n",
    "        return self.data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d83d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeEstimator:\n",
    "    \"\"\"A Decision Tree Estimator\"\"\"\n",
    "    def __init__(self, criterion={}):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        root: the starting node of the decision tree\n",
    "        n_levels: contains a list of all unique feature values for each descriptive feature\n",
    "        criterion (pre-pruning): {max_depth, partition_threshold, low_gain}\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.n_levels = None\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __repr__(self, node=None):\n",
    "        \"\"\"Displays the decision tree (Pre-Order Traversal)\"\"\"\n",
    "        if not node:\n",
    "            node = self.root\n",
    "        return str(node) + ''.join(['\\n' + self.__repr__(child) for child in node.children])\n",
    "\n",
    "    def partition(self, X, y, d, t):\n",
    "        \"\"\"Returns a subset of the training data with feature (d) of level (t)\"\"\"\n",
    "        D = pd.concat([X.loc[X[d]==t], y.loc[X[d]==t]], axis=1)\n",
    "        D = D.drop([d], axis=1)\n",
    "        return D.iloc[:, :-1], D.iloc[:, -1], t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_levels = {d: X[d].unique() for d in X.columns}\n",
    "        self.root = self.make_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            for child in node.children:\n",
    "                if child.branch == x.get(node.feature).values:\n",
    "                    node = child\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Branch {child.feature} -> {x.branch} does not exist\")\n",
    "        return node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return [self.predict(X.iloc[x].to_frame().T).feature for x in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03787b5",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "###  Gini Index\n",
    "$Gini(t, \\mathcal{D})=1-\\sum_{l\\in levels(t)}P(t=l)^2$\n",
    "\n",
    "### Entropy *(Bits)*\n",
    "$\\mathcal{H}(t, \\mathcal{D})=-\\sum_{l\\in levels(t)\\\\}^{}{P(t=l)\\cdot\\log_2(P(t=l))}$\n",
    "\n",
    "### Rem\n",
    "$rem(d,\\mathcal{D})=\\sum_{l\\in levels(t)}{}\\frac{|\\mathcal{D}_{d=l}|}{\\mathcal{D}}\\cdot \\mathcal{H}(t, \\mathcal{D}_{d=l})$\n",
    "\n",
    "### Information Gain\n",
    "$IG(d, \\mathcal{D})=\\mathcal{H}(t, \\mathcal{D})-rem(d, \\mathcal{D})$\n",
    "\n",
    "### Information Gain Ratio\n",
    "$GR(d, \\mathcal{D})=\\frac{IG(d, \\mathcal{D})}{\\mathcal{H}(d, \\mathcal{D})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ed769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Classifier\"\"\"\n",
    "    def __init__(self, *, metric=\"entropy\", eval=\"info_gain\", criterion={}):\n",
    "        \"\"\"\n",
    "        Metric: {gain, gini}\n",
    "        Eval: {info_gain, gain_ratio}\n",
    "        \"\"\"\n",
    "        super().__init__(criterion)\n",
    "        self.metric = self.entropy if metric == \"entropy\" else self.gini\n",
    "        self.eval = self.information_gain if eval == \"info_gain\" else self.information_gain_ratio\n",
    "\n",
    "    def gini(self, X, y):\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return 1 - np.sum([proba(t)**2 for t in y.unique()])\n",
    "\n",
    "    def entropy(self, X, y):\n",
    "        \"\"\"Measures the amount of uncertainty/impurity/heterogeneity in (X, y)\"\"\"\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return -np.sum([proba(t) * np.log2(proba(t)) for t in y.unique()])\n",
    "\n",
    "    def rem(self, X, y, d):\n",
    "        \"\"\"Measures the entropy after partitioning (X, y) on feature (d)\"\"\"\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.metric(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def information_gain(self, X, y, d):\n",
    "        \"\"\"Measures the reduction in the overall entropy in (X, y) achieved by testing on feature (d)\"\"\"\n",
    "        if debug:\n",
    "            print(f\"{d} = {self.metric(X, y):.3f} - {self.rem(X, y, d):.3f} = {self.metric(X, y) - self.rem(X, y, d):.3f}\") \n",
    "        return self.metric(X, y) - self.rem(X, y, d)\n",
    "\n",
    "    def information_gain_ratio(self, X, y, d):\n",
    "        proba = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        entropy = lambda: -np.sum([proba(t) * np.log2(proba(t)) for t in X[d].unique()])\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{d} = ({self.metric(X, y):.3f} - {self.rem(X, y, d):.3f}) / {entropy()} = {(self.metric(X, y) - self.rem(X, y, d)) / entropy()}\")\n",
    "        return self.metric(X, y) - self.rem(X, y, d) / entropy()\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"Performs the ID3 algorithm\"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:  # all instances have the same target feature values\n",
    "            if debug:\n",
    "                print(\"All instances have the same target feature value\\n\")\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:  # dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "            if debug:\n",
    "                print(\"Dataset is empty\\n\")\n",
    "            return make_node(mode(parent.y), True)\n",
    "        elif all((X[d] == X[d].iloc[0]).all() for d in X.columns):  # if all feature values are identical\n",
    "            if debug:\n",
    "                print(\"All instances have the same descriptive features\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        elif self.criterion.get(\"max_depth\", float('inf')) <= depth:  # max depth reached\n",
    "            if debug:\n",
    "                print(\"Stopping at Max Depth\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        elif self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):  # max number of instances in partitioned dataset reached\n",
    "            if debug:\n",
    "                print(f\"Stopping at {len(X)} instances\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"===Information Gain===\")\n",
    "\n",
    "        max_gain = np.argmax([self.eval(X, y, d) for d in X.columns])\n",
    "\n",
    "        if self.criterion.get('low_gain', float('-inf')) >= max_gain:\n",
    "            if debug:\n",
    "                print(f\"Stopping at Gain={max_gain}\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        best_feature = X.columns[max_gain]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        if debug:\n",
    "            print()\n",
    "            print(\"===Best Feature===\")\n",
    "            print(best_feature)\n",
    "            print()\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            if debug:\n",
    "                print(f\"===Partitioned Dataset ({level})===\")\n",
    "                print(pd.concat(d, axis=1).head())\n",
    "                print()\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96c2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dt_clf = DecisionTreeClassifier(metric='entropy', criterion={'partition_threshold': len(X_train) * 5e-2}).fit(X_train, y_train)\n",
    "# dt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a89d52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(metric='entropy').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e4b990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Elevation (Branch=None)\n",
       "\t Slope (Branch=high)\n",
       "\t\t chapparal (Branch=steep)\n",
       "\t\t chapparal (Branch=moderate)\n",
       "\t\t conifer (Branch=flat)\n",
       "\t riparian (Branch=low)\n",
       "\t Stream (Branch=medium)\n",
       "\t\t chapparal (Branch=false)\n",
       "\t\t riparian (Branch=true)\n",
       "\t conifer (Branch=highest)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72776ddb",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "$var(t, \\mathcal{D})=\\frac{\\sum_{i=1}^n(t_i-\\bar{t})^2}{n-1}$\n",
    "\n",
    "$weighted\\ var(t, \\mathcal{D}) = \\sum_{l\\in levels(d)}{} \\frac{|\\mathcal{D}_{d=l}|}{|\\mathcal{D}|} \\times var(t, \\mathcal{D}_{d=l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4197bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Regressor\"\"\"\n",
    "    def __init__(self, *, metric=\"variance\", criterion={}):\n",
    "        \"\"\"\n",
    "        Metric = {variance}\n",
    "        \"\"\"\n",
    "        super().__init__(criterion)\n",
    "        self.metric = metric\n",
    "\n",
    "    def variance(self, X, y, d):\n",
    "        if len(X) == 1:\n",
    "            return 0\n",
    "        if debug:\n",
    "            print(f\"{d} = {np.sum([(t-mean(y))**2 for t in y]) / (len(X)-1)}\")\n",
    "        return np.sum([(t-mean(y))**2 for t in y]) / (len(X)-1)\n",
    "\n",
    "    def weighted_variance(self, X, y, d):\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.variance(X.loc[X[d]==t], y.loc[X[d]==t], d) for t in X[d].unique()])\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"Performs the ID3 algorithm\"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:  # all instances have the same target feature values\n",
    "            if debug:\n",
    "                print(\"All instances have the same target feature value\\n\")\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:  # dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "            if debug:\n",
    "                print(\"Dataset is empty\\n\")\n",
    "            return make_node(mean(y), True)\n",
    "        elif self.criterion.get(\"max_depth\", float('inf')) <= depth:\n",
    "            if debug:\n",
    "                print(\"Stopping at Max Depth\\n\")\n",
    "            return make_node(mean(y), True)\n",
    "        elif self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):\n",
    "            if debug:\n",
    "                print(f\"Stopping at {len(X)} instances\\n\")\n",
    "            return make_node(mean(y), True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"===Variance===\")\n",
    "\n",
    "        min_var = np.argmin([self.weighted_variance(X, y, d) for d in X.columns])\n",
    "\n",
    "        best_feature = X.columns[min_var]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        if debug:\n",
    "            print()\n",
    "            print(\"===Best Feature===\")\n",
    "            print(best_feature)\n",
    "            print()\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            if debug:\n",
    "                print(f\"===Partitioned Dataset ({level})===\")\n",
    "                print(pd.concat(d, axis=1).head())\n",
    "                print()\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return mean_squared_error(y, y_hat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee157954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Season': ['winter', 'winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'autumn', 'autumn', 'autumn'],\n",
    "    'Work Day': ['false', 'false', 'true', 'false', 'true', 'true', 'false', 'true', 'true', 'false', 'false', 'true'],\n",
    "    'Rentals': [800, 826, 900, 2100, 4740, 4900, 3000, 5800, 6200, 2910, 2880, 2820],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "A, b = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e6224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_regr = DecisionTreeRegressor().fit(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9fedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Season (Branch=None)\n",
       "\t Work Day (Branch=winter)\n",
       "\t\t 813 (Branch=false)\n",
       "\t\t 900 (Branch=true)\n",
       "\t Work Day (Branch=spring)\n",
       "\t\t 2100 (Branch=false)\n",
       "\t\t 4820 (Branch=true)\n",
       "\t Work Day (Branch=summer)\n",
       "\t\t 3000 (Branch=false)\n",
       "\t\t 6000 (Branch=true)\n",
       "\t Work Day (Branch=autumn)\n",
       "\t\t 2895 (Branch=false)\n",
       "\t\t 2820 (Branch=true)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885a388",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "454ac107-8855-4862-8582-e1f55e2f6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=5, n_sample=2, eval=\"info_gain\", criterion={}):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_sample = n_sample\n",
    "        self.forest = [DecisionTreeClassifier(eval=eval, criterion=criterion) for _ in range(n_estimators)]\n",
    "\n",
    "    def sub_sample(self, X, n_sample=2):\n",
    "        \"\"\"Enforces feature randomness\"\"\"\n",
    "        return np.random.choice(X.columns.to_numpy(), n_sample, replace=False)\n",
    "\n",
    "    def bootstrap_sample(self, X, y, n_sample, key=True):\n",
    "        feature_subset = self.sub_sample(X, int(np.log2(len(X))))\n",
    "        d = pd.concat([X, y], axis=1)\n",
    "        d = d.sample(n=n_sample, replace=key)\n",
    "        return d.iloc[:, :-1][feature_subset], d.iloc[:, -1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _, tree in enumerate(self.forest):\n",
    "            print(f\"Decision Tree #{_}\")\n",
    "            tree.fit(*self.bootstrap_sample(X, y, self.n_sample))\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        assert all(isinstance(model, DecisionTreeClassifier) for model in self.forest)\n",
    "        return mode([dt.predict(x).feature for dt in self.forest])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = [self.predict(X.iloc[x].to_frame().T) for x in range(len(X))]\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d17d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rf = RandomForest(n_estimators=1000, n_sample=len(X_train), criterion={'partition_threshold': len(X_train) * 5e-2}).fit(X_train, y_train)\n",
    "# rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b106406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree #0\n",
      "Decision Tree #1\n",
      "Decision Tree #2\n",
      "Decision Tree #3\n",
      "Decision Tree #4\n",
      "Decision Tree #5\n",
      "Decision Tree #6\n",
      "Decision Tree #7\n",
      "Decision Tree #8\n",
      "Decision Tree #9\n",
      "Decision Tree #10\n",
      "Decision Tree #11\n",
      "Decision Tree #12\n",
      "Decision Tree #13\n",
      "Decision Tree #14\n",
      "Decision Tree #15\n",
      "Decision Tree #16\n",
      "Decision Tree #17\n",
      "Decision Tree #18\n",
      "Decision Tree #19\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(n_estimators=20, n_sample=len(X)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20455692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
