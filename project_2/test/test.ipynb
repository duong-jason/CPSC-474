{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbfe8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from statistics import mode, mean\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c582c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8501821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"/Users/duong-jason/Desktop/dc/project_2/test/cancer.pkl\")\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0a4a2",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288856b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A Decision Tree Node\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature - the value of a descriptive/target feature of a node\n",
    "    data - the partitioned dataset resulting from the parent node on a feature value\n",
    "    branch - the feature value from the parent node\n",
    "    parent - the immediate adjacent node along the path from the root\n",
    "    leaf - denotes a terminal node whose prediction is based on the path from the root to the node\n",
    "    depth - the number of levels from the root to a node\n",
    "    children - the nodes resulting from each unique feature value of the parent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        feature=None,\n",
    "        data=None,\n",
    "        branch=None,\n",
    "        parent=None,\n",
    "        leaf=False,\n",
    "        depth=0,\n",
    "        children=[]\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.data = data\n",
    "        self.branch = branch\n",
    "        self.parent = parent\n",
    "        self.leaf = leaf\n",
    "        self.depth = depth\n",
    "        self.children = children\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.depth * '\\t' + f\" {self.feature} (Branch={self.branch})\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Returns whether a node is terminal\"\"\"\n",
    "        return self.leaf\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Returns the partitioned feature matrix of a node\"\"\"\n",
    "        return self.data.iloc[:, :-1]\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"Returns the partitioned target vector of a node\"\"\"\n",
    "        return self.data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d83d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeEstimator:\n",
    "    \"\"\"A Decision Tree Estimator\"\"\"\n",
    "    def __init__(self, criterion={}):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        root: the starting node of the decision tree\n",
    "        n_levels: contains a list of all unique feature values for each descriptive feature\n",
    "        criterion (pre-pruning): {max_depth, partition_threshold, low_gain}\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.n_levels = None\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __repr__(self, node=None):\n",
    "        \"\"\"Displays the decision tree (Pre-Order Traversal)\"\"\"\n",
    "        if not node:\n",
    "            node = self.root\n",
    "        return str(node) + ''.join(['\\n' + self.__repr__(child) for child in node.children])\n",
    "\n",
    "    def partition(self, X, y, d, t):\n",
    "        \"\"\"Returns a subset of the training data with feature (d) of level (t)\"\"\"\n",
    "        D = pd.concat([X.loc[X[d]==t], y.loc[X[d]==t]], axis=1)\n",
    "        D = D.drop([d], axis=1)\n",
    "        return D.iloc[:, :-1], D.iloc[:, -1], t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_levels = {d: X[d].unique() for d in X.columns}\n",
    "        self.root = self.make_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            for child in node.children:\n",
    "                if child.branch == x.get(node.feature).values:\n",
    "                    node = child\n",
    "                    break\n",
    "            else:\n",
    "                # return Node(feature=mode(node.y),\n",
    "                #             data=pd.concat([node.X, node.y], axis=1),\n",
    "                #             branch=node.branch,\n",
    "                #             parent=node.parent,\n",
    "                #             depth=node.depth)\n",
    "                raise ValueError(f\"Branch {node.feature} -> {child.branch} does not exist\")\n",
    "        return node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return [self.predict(X.iloc[x].to_frame().T).feature for x in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03787b5",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "###  Gini Index\n",
    "$Gini(t, \\mathcal{D})=1-\\sum_{l\\in levels(t)}P(t=l)^2$\n",
    "\n",
    "### Entropy *(Bits)*\n",
    "$\\mathcal{H}(t, \\mathcal{D})=-\\sum_{l\\in levels(t)\\\\}^{}{P(t=l)\\cdot\\log_2(P(t=l))}$\n",
    "\n",
    "### Rem\n",
    "$rem(d,\\mathcal{D})=\\sum_{l\\in levels(t)}{}\\frac{|\\mathcal{D}_{d=l}|}{\\mathcal{D}}\\cdot \\mathcal{H}(t, \\mathcal{D}_{d=l})$\n",
    "\n",
    "### Information Gain\n",
    "$IG(d, \\mathcal{D})=\\mathcal{H}(t, \\mathcal{D})-rem(d, \\mathcal{D})$\n",
    "\n",
    "### Information Gain Ratio\n",
    "$GR(d, \\mathcal{D})=\\frac{IG(d, \\mathcal{D})}{\\mathcal{H}(d, \\mathcal{D})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ed769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Classifier\"\"\"\n",
    "    def __init__(self, *, metric=\"entropy\", eval=\"info_gain\", criterion={}):\n",
    "        \"\"\"\n",
    "        Metric: {gain, gini}\n",
    "        Eval: {info_gain, gain_ratio}\n",
    "        \"\"\"\n",
    "        super().__init__(criterion)\n",
    "        self.metric = self.entropy if metric == \"entropy\" else self.gini\n",
    "        self.eval = self.information_gain if eval == \"info_gain\" else self.information_gain_ratio\n",
    "\n",
    "    def gini(self, X, y):\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return 1 - np.sum([proba(t)**2 for t in y.unique()])\n",
    "\n",
    "    def entropy(self, X, y):\n",
    "        \"\"\"Measures the amount of uncertainty/impurity/heterogeneity in (X, y)\"\"\"\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return -np.sum([proba(t) * np.log2(proba(t)) for t in y.unique()])\n",
    "\n",
    "    def rem(self, X, y, d):\n",
    "        \"\"\"Measures the entropy after partitioning (X, y) on feature (d)\"\"\"\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.metric(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def information_gain(self, X, y, d):\n",
    "        \"\"\"Measures the reduction in the overall entropy in (X, y) achieved by testing on feature (d)\"\"\"\n",
    "        if debug:\n",
    "            print(f\"{d} = {self.metric(X, y):.3f} - {self.rem(X, y, d):.3f} = {self.metric(X, y) - self.rem(X, y, d):.3f}\") \n",
    "        return self.metric(X, y) - self.rem(X, y, d)\n",
    "\n",
    "    def information_gain_ratio(self, X, y, d):\n",
    "        proba = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        entropy = lambda: -np.sum([proba(t) * np.log2(proba(t)) for t in X[d].unique()])\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{d} = ({self.metric(X, y):.3f} - {self.rem(X, y, d):.3f}) / {entropy()} = {(self.metric(X, y) - self.rem(X, y, d)) / entropy()}\")\n",
    "        return self.metric(X, y) - self.rem(X, y, d) / entropy()\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"Performs the ID3 algorithm\"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:  # all instances have the same target feature values\n",
    "            if debug:\n",
    "                print(\"All instances have the same target feature value\\n\")\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:  # dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "            if debug:\n",
    "                print(\"Dataset is empty\\n\")\n",
    "            return make_node(mode(parent.y), True)\n",
    "        elif all((X[d] == X[d].iloc[0]).all() for d in X.columns):  # if all feature values are identical\n",
    "            if debug:\n",
    "                print(\"All instances have the same descriptive features\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        if self.criterion.get(\"max_depth\", float('inf')) <= depth:  # max depth reached\n",
    "            if debug:\n",
    "                print(\"Stopping at Max Depth\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        if self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):  # max number of instances in partitioned dataset reached\n",
    "            if debug:\n",
    "                print(f\"Stopping at {len(X)} instances\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"===Information Gain===\")\n",
    "\n",
    "        max_gain = np.argmax([self.eval(X, y, d) for d in X.columns])\n",
    "\n",
    "        if self.criterion.get('low_gain', float('-inf')) >= max_gain:\n",
    "            if debug:\n",
    "                print(f\"Stopping at Gain={max_gain}\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        best_feature = X.columns[max_gain]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        if debug:\n",
    "            print()\n",
    "            print(\"===Best Feature===\")\n",
    "            print(best_feature)\n",
    "            print()\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            if debug:\n",
    "                print(f\"===Partitioned Dataset ({level})===\")\n",
    "                print(pd.concat(d, axis=1).head())\n",
    "                print()\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96c2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 11.3 ms, total: 1.48 s\n",
      "Wall time: 1.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[116,   5],\n",
       "       [  5,  62]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_clf = DecisionTreeClassifier(metric='entropy', criterion={'partition_threshold': len(X_train) * 5e-2}).fit(X_train, y_train)\n",
    "dt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e4b990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " worst perimeter (Branch=None)\n",
       "\t worst symmetry (Branch=101.64999771118164)\n",
       "\t\t 1 (Branch=0.3570999950170517)\n",
       "\t\t 1 (Branch=0.2639999985694885)\n",
       "\t\t 1 (Branch=0.2992500066757202)\n",
       "\t\t 1 (Branch=0.2806500047445297)\n",
       "\t\t 1 (Branch=0.31745000183582306)\n",
       "\t\t 1 (Branch=0.2236500009894371)\n",
       "\t\t 1 (Branch=0.243599995970726)\n",
       "\t\t 1 (Branch=0.3874499946832657)\n",
       "\t\t 0 (Branch=0.6638)\n",
       "\t 1 (Branch=87.36999893188477)\n",
       "\t worst texture (Branch=117.44999694824219)\n",
       "\t\t 1 (Branch=19.109999656677246)\n",
       "\t\t 0 (Branch=21.829999923706055)\n",
       "\t\t 0 (Branch=23.34999942779541)\n",
       "\t\t 1 (Branch=20.84500026702881)\n",
       "\t\t 1 (Branch=25.829999923706055)\n",
       "\t\t 0 (Branch=24.869999885559082)\n",
       "\t\t 0 (Branch=29.295000076293945)\n",
       "\t\t 0 (Branch=27.22499942779541)\n",
       "\t\t 0 (Branch=49.54)\n",
       "\t\t 0 (Branch=32.310001373291016)\n",
       "\t fractal dimension error (Branch=251.2)\n",
       "\t\t 0 (Branch=0.0031159999780356884)\n",
       "\t\t 0 (Branch=0.0023330000694841146)\n",
       "\t\t 0 (Branch=0.002572000026702881)\n",
       "\t\t 0 (Branch=0.004211000166833401)\n",
       "\t\t 1 (Branch=0.0015705000259913504)\n",
       "\t\t 0 (Branch=0.006436000112444162)\n",
       "\t\t 0 (Branch=0.004571500001475215)\n",
       "\t\t 0 (Branch=0.0037239999510347843)\n",
       "\t\t 0 (Branch=0.02984)\n",
       "\t worst texture (Branch=105.95000076293945)\n",
       "\t\t 1 (Branch=19.109999656677246)\n",
       "\t\t 1 (Branch=21.829999923706055)\n",
       "\t\t 1 (Branch=23.34999942779541)\n",
       "\t\t 1 (Branch=20.84500026702881)\n",
       "\t\t 1 (Branch=25.829999923706055)\n",
       "\t\t 0 (Branch=24.869999885559082)\n",
       "\t\t 1 (Branch=29.295000076293945)\n",
       "\t\t 0 (Branch=27.22499942779541)\n",
       "\t\t 0 (Branch=49.54)\n",
       "\t\t 0 (Branch=32.310001373291016)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72776ddb",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "$var(t, \\mathcal{D})=\\frac{\\sum_{i=1}^n(t_i-\\bar{t})^2}{n-1}$\n",
    "\n",
    "$weighted\\ var(t, \\mathcal{D}) = \\sum_{l\\in levels(d)}{} \\frac{|\\mathcal{D}_{d=l}|}{|\\mathcal{D}|} \\times var(t, \\mathcal{D}_{d=l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4197bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Regressor\"\"\"\n",
    "    def __init__(self, *, criterion={}):\n",
    "        super().__init__(criterion)\n",
    "\n",
    "    def variance(self, X, y):\n",
    "        if len(X) == 1:\n",
    "            return 0\n",
    "        return np.sum([(t-mean(y))**2 for t in y]) / (len(X)-1)\n",
    "\n",
    "    def weighted_variance(self, X, y, d):\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.variance(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"\n",
    "        Performs the ID3 algorithm\n",
    "\n",
    "        Base Cases\n",
    "        ----------\n",
    "        - all instances have the same target feature values\n",
    "        - dataset is empty, return a leaf node labeled with the majority class\n",
    "        - max_depth reached\n",
    "        - max number of instances in partitioned dataset reached\n",
    "        \"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:\n",
    "            return make_node(mean(y), True)\n",
    "        if self.criterion.get(\"max_depth\", float('inf')) <= depth:\n",
    "            return make_node(mean(y), True)\n",
    "        if self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):\n",
    "            return make_node(mean(y), True)\n",
    "\n",
    "        min_var = np.argmin([self.weighted_variance(X, y, d) for d in X.columns])\n",
    "\n",
    "        best_feature = X.columns[min_var]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return mean_squared_error(y, y_hat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee157954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Season': ['winter', 'winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'autumn', 'autumn', 'autumn'],\n",
    "    'Work Day': ['false', 'false', 'true', 'false', 'true', 'true', 'false', 'true', 'true', 'false', 'false', 'true'],\n",
    "    'Rentals': [800, 826, 900, 2100, 4740, 4900, 3000, 5800, 6200, 2910, 2880, 2820],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "A, b = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e6224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_regr = DecisionTreeRegressor().fit(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9fedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Season (Branch=None)\n",
       "\t Work Day (Branch=winter)\n",
       "\t\t 813 (Branch=false)\n",
       "\t\t 900 (Branch=true)\n",
       "\t Work Day (Branch=spring)\n",
       "\t\t 2100 (Branch=false)\n",
       "\t\t 4820 (Branch=true)\n",
       "\t Work Day (Branch=summer)\n",
       "\t\t 3000 (Branch=false)\n",
       "\t\t 6000 (Branch=true)\n",
       "\t Work Day (Branch=autumn)\n",
       "\t\t 2895 (Branch=false)\n",
       "\t\t 2820 (Branch=true)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885a388",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454ac107-8855-4862-8582-e1f55e2f6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=5, n_sample=0, eval=\"info_gain\", criterion={}):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_sample = n_sample\n",
    "        self.forest = [DecisionTreeClassifier(eval=eval, criterion=criterion) for _ in range(n_estimators)]\n",
    "\n",
    "    def sub_sample(self, X, n_sample=2):\n",
    "        \"\"\"Enforces feature randomness\"\"\"\n",
    "        return np.random.choice(X.columns.to_numpy(), n_sample, replace=False)\n",
    "\n",
    "    def bootstrap_sample(self, X, y, n_sample, key=True):\n",
    "        feature_subset = self.sub_sample(X, int(np.log2(len(X))))\n",
    "        d = pd.concat([X, y], axis=1)\n",
    "        d = d.sample(n=n_sample, replace=key)\n",
    "        return d.iloc[:, :-1][feature_subset], d.iloc[:, -1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _, tree in enumerate(self.forest):\n",
    "            print(f\"Decision Tree #{_}\")\n",
    "            tree.fit(*self.bootstrap_sample(X, y, self.n_sample))\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        assert all(isinstance(model, DecisionTreeClassifier) for model in self.forest)\n",
    "        return mode([dt.predict(x).feature for dt in self.forest])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = [self.predict(X.iloc[x].to_frame().T) for x in range(len(X))]\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d17d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree #0\n",
      "Decision Tree #1\n",
      "Decision Tree #2\n",
      "Decision Tree #3\n",
      "Decision Tree #4\n",
      "Decision Tree #5\n",
      "Decision Tree #6\n",
      "Decision Tree #7\n",
      "Decision Tree #8\n",
      "Decision Tree #9\n",
      "CPU times: user 4.28 s, sys: 73.5 ms, total: 4.35 s\n",
      "Wall time: 4.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[117,   4],\n",
       "       [  7,  60]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForest(n_estimators=10, n_sample=len(X_train), criterion={'max_depth': 4}).fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f75164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
