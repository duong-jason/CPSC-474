{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cbfe8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from statistics import mode, mean\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c582c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8501821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"/Users/duong-jason/CPSC-474/project_2/test/cancer.pkl\")\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0a4a2",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "288856b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A Decision Tree Node\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature - the value of a descriptive/target feature of a node\n",
    "    data - the partitioned dataset resulting from the parent node on a feature value\n",
    "    branch - the feature value from the parent node\n",
    "    parent - the immediate adjacent node along the path from the root\n",
    "    leaf - denotes a terminal node whose prediction is based on the path from the root to the node\n",
    "    depth - the number of levels from the root to a node\n",
    "    children - the nodes resulting from each unique feature value of the parent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        feature=None,\n",
    "        data=None,\n",
    "        branch=None,\n",
    "        parent=None,\n",
    "        leaf=False,\n",
    "        depth=0,\n",
    "        children=[]\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.data = data\n",
    "        self.branch = branch\n",
    "        self.parent = parent\n",
    "        self.leaf = leaf\n",
    "        self.depth = depth\n",
    "        self.children = children\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.depth * '\\t' + f\"{self.feature} (Branch={self.branch})\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Returns whether a node is terminal\"\"\"\n",
    "        return self.leaf\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"Returns the partitioned feature matrix of a node\"\"\"\n",
    "        return self.data.iloc[:, :-1]\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"Returns the partitioned target vector of a node\"\"\"\n",
    "        return self.data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57d83d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeEstimator:\n",
    "    \"\"\"A Decision Tree Estimator\"\"\"\n",
    "    def __init__(self, criterion={}):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        root: the starting node of the decision tree\n",
    "        n_levels: contains a list of all unique feature values for each descriptive feature\n",
    "        criterion (pre-pruning): {max_depth, partition_threshold, low_gain}\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.n_levels = None\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __repr__(self, node=None):\n",
    "        \"\"\"Displays the decision tree (Pre-Order Traversal)\"\"\"\n",
    "        if not node:\n",
    "            node = self.root\n",
    "        return str(node) + ''.join(['\\n' + self.__repr__(child) for child in node.children])\n",
    "\n",
    "    def partition(self, X, y, d, t):\n",
    "        \"\"\"Returns a subset of the training data with feature (d) with level (t)\"\"\"\n",
    "        D = pd.concat([X.loc[X[d]==t], y.loc[X[d]==t]], axis=1)\n",
    "        D = D.drop([d], axis=1)\n",
    "        return D.iloc[:, :-1], D.iloc[:, -1], t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_levels = {d: X[d].unique() for d in X.columns}\n",
    "        self.root = self.make_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            for child in node.children:\n",
    "                if child.branch == x.get(node.feature).values:\n",
    "                    node = child\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Branch {node.feature} -> {x.get(node.feature).values} does not exist\")\n",
    "        return node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return [self.predict(X.iloc[x].to_frame().T).feature for x in range(len(X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03787b5",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "###  Gini Index\n",
    "$Gini(t, \\mathcal{D})=1-\\sum_{l\\in levels(t)}P(t=l)^2$\n",
    "\n",
    "### Entropy *(Bits)*\n",
    "$\\mathcal{H}(t, \\mathcal{D})=-\\sum_{l\\in levels(t)\\\\}^{}{P(t=l)\\cdot\\log_2(P(t=l))}$\n",
    "\n",
    "### Rem\n",
    "$rem(d,\\mathcal{D})=\\sum_{l\\in levels(t)}{}\\frac{|\\mathcal{D}_{d=l}|}{\\mathcal{D}}\\cdot \\mathcal{H}(t, \\mathcal{D}_{d=l})$\n",
    "\n",
    "### Information Gain\n",
    "$IG(d, \\mathcal{D})=\\mathcal{H}(t, \\mathcal{D})-rem(d, \\mathcal{D})$\n",
    "\n",
    "### Information Gain Ratio\n",
    "$GR(d, \\mathcal{D})=\\frac{IG(d, \\mathcal{D})}{\\mathcal{H}(d, \\mathcal{D})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65ed769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Classifier\"\"\"\n",
    "    def __init__(self, *, metric=\"entropy\", eval=\"info_gain\", criterion={}):\n",
    "        \"\"\"\n",
    "        Metric: {gain, gini}\n",
    "        Eval: {info_gain, gain_ratio}\n",
    "        \"\"\"\n",
    "        super().__init__(criterion)\n",
    "        self.metric = self.entropy if metric == \"entropy\" else self.gini\n",
    "        self.eval = self.information_gain if eval == \"info_gain\" else self.information_gain_ratio\n",
    "\n",
    "    def gini(self, X, y):\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return 1 - np.sum([proba(t)**2 for t in y.unique()])\n",
    "\n",
    "    def entropy(self, X, y):\n",
    "        \"\"\"Measures the amount of uncertainty/impurity/heterogeneity in (X, y)\"\"\"\n",
    "        proba = lambda t: len(X.loc[y==t]) / len(X)\n",
    "        return -np.sum([proba(t) * np.log2(proba(t)) for t in y.unique()])\n",
    "\n",
    "    def rem(self, X, y, d):\n",
    "        \"\"\"Measures the entropy after partitioning (X, y) on feature (d)\"\"\"\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.metric(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def information_gain(self, X, y, d):\n",
    "        \"\"\"Measures the reduction in the overall entropy in (X, y) achieved by testing on feature (d)\"\"\"\n",
    "        if debug:\n",
    "            print(f\"{d} = {self.metric(X, y):.3f} - {self.rem(X, y, d):.3f} = {self.metric(X, y) - self.rem(X, y, d):.3f}\") \n",
    "        return self.metric(X, y) - self.rem(X, y, d)\n",
    "\n",
    "    def information_gain_ratio(self, X, y, d):\n",
    "        proba = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        entropy = lambda: -np.sum([proba(t) * np.log2(proba(t)) for t in X[d].unique()])\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{d} = ({self.metric(X, y):.3f} - {self.rem(X, y, d):.3f}) / {entropy()} = {(self.metric(X, y) - self.rem(X, y, d)) / entropy()}\")\n",
    "        return self.metric(X, y) - self.rem(X, y, d) / entropy()\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"Performs the ID3 algorithm\n",
    "\n",
    "        Base Cases\n",
    "        ----------\n",
    "        - all instances have the same target feature values\n",
    "        - dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "        - if all feature values are identical\n",
    "        - max_depth reached\n",
    "        - max number of instances in partitioned dataset reached\n",
    "        \"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:  # all instances have the same target feature values\n",
    "            if debug:\n",
    "                print(\"All instances have the same target feature value\\n\")\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:  # dataset is empty, return a leaf node labeled with the majority class of the parent\n",
    "            if debug:\n",
    "                print(\"Dataset is empty\\n\")\n",
    "            return make_node(mode(parent.y), True)\n",
    "        elif all((X[d] == X[d].iloc[0]).all() for d in X.columns):  # if all feature values are identical\n",
    "            if debug:\n",
    "                print(\"All instances have the same descriptive features\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        if self.criterion.get(\"max_depth\", float('inf')) <= depth:  # max depth reached\n",
    "            if debug:\n",
    "                print(\"Stopping at Max Depth\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "        if self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):  # max number of instances in partitioned dataset reached\n",
    "            if debug:\n",
    "                print(f\"Stopping at {len(X)} instances\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"===Information Gain===\")\n",
    "\n",
    "        max_gain = np.argmax([self.eval(X, y, d) for d in X.columns])\n",
    "\n",
    "        if self.criterion.get('low_gain', float('-inf')) >= max_gain:\n",
    "            if debug:\n",
    "                print(f\"Stopping at Gain={max_gain}\\n\")\n",
    "            return make_node(mode(y), True)\n",
    "\n",
    "        best_feature = X.columns[max_gain]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        if debug:\n",
    "            print()\n",
    "            print(\"===Best Feature===\")\n",
    "            print(best_feature)\n",
    "            print()\n",
    "\n",
    "        # X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in X[best_feature].unique()]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            if debug:\n",
    "                print(f\"===Partitioned Dataset ({level})===\")\n",
    "                print(pd.concat(d, axis=1).head())\n",
    "                print()\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "430f67e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  y\n",
       "0  0  0  0  0\n",
       "1  1  1  1  1\n",
       "2  1  0  2  1\n",
       "3  0  0  2  0\n",
       "4  0  2  0  2\n",
       "5  1  0  4  2\n",
       "6  1  0  0  0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug=1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'A': [0, 1, 1, 0, 0, 1, 1],\n",
    "    'B': [0, 1, 0, 0, 2, 0, 0],\n",
    "    'C': [0, 1, 2, 2, 0, 4, 0],\n",
    "    'y': [0, 1, 1, 0, 2, 2, 0],\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58a15298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Information Gain===\n",
      "A = 1.557 - 1.251 = 0.306\n",
      "B = 1.557 - 0.979 = 0.577\n",
      "C = 1.557 - 0.679 = 0.877\n",
      "\n",
      "===Best Feature===\n",
      "C\n",
      "\n",
      "===Partitioned Dataset (0)===\n",
      "   A  B  y\n",
      "0  0  0  0\n",
      "4  0  2  2\n",
      "6  1  0  0\n",
      "\n",
      "===Information Gain===\n",
      "A = 0.918 - 0.667 = 0.252\n",
      "B = 0.918 - 0.000 = 0.918\n",
      "\n",
      "===Best Feature===\n",
      "B\n",
      "\n",
      "===Partitioned Dataset (0)===\n",
      "   A  y\n",
      "0  0  0\n",
      "6  1  0\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "===Partitioned Dataset (2)===\n",
      "   A  y\n",
      "4  0  2\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "===Partitioned Dataset (1)===\n",
      "   A  B  y\n",
      "1  1  1  1\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "===Partitioned Dataset (2)===\n",
      "   A  B  y\n",
      "2  1  0  1\n",
      "3  0  0  0\n",
      "\n",
      "===Information Gain===\n",
      "A = 1.000 - 0.000 = 1.000\n",
      "B = 1.000 - 1.000 = 0.000\n",
      "\n",
      "===Best Feature===\n",
      "A\n",
      "\n",
      "===Partitioned Dataset (1)===\n",
      "   B  y\n",
      "2  0  1\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "===Partitioned Dataset (0)===\n",
      "   B  y\n",
      "3  0  0\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "===Partitioned Dataset (4)===\n",
      "   A  B  y\n",
      "5  1  0  2\n",
      "\n",
      "All instances have the same target feature value\n",
      "\n",
      "C (Branch=None)\n",
      "\tB (Branch=0)\n",
      "\t\t0 (Branch=0)\n",
      "\t\t2 (Branch=2)\n",
      "\t1 (Branch=1)\n",
      "\tA (Branch=2)\n",
      "\t\t1 (Branch=1)\n",
      "\t\t0 (Branch=0)\n",
      "\t2 (Branch=4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Branch B -> [1] does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/duong-jason/CPSC-474/project_2/test/test.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m query \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m]})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m X_t, y_t \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39miloc[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], query\u001b[39m.\u001b[39miloc[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m y_hat \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39;49mpredict(X_t)\u001b[39m.\u001b[39mfeature\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction:\u001b[39m\u001b[39m\"\u001b[39m, y_hat)\n",
      "\u001b[1;32m/Users/duong-jason/CPSC-474/project_2/test/test.ipynb Cell 10\u001b[0m in \u001b[0;36mDecisionTreeEstimator.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBranch \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mfeature\u001b[39m}\u001b[39;00m\u001b[39m -> \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mget(node\u001b[39m.\u001b[39mfeature)\u001b[39m.\u001b[39mvalues\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.24/Users/duong-jason/CPSC-474/project_2/test/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "\u001b[0;31mValueError\u001b[0m: Branch B -> [1] does not exist"
     ]
    }
   ],
   "source": [
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "dt = DecisionTreeClassifier(metric=\"entropy\")\n",
    "dt.fit(X, y)\n",
    "print(dt)\n",
    "\n",
    "query = pd.DataFrame({'A': [0], 'B': [1], 'C': [0], 'y': [0]})\n",
    "X_t, y_t = query.iloc[:, :-1], query.iloc[:, -1]\n",
    "\n",
    "y_hat = dt.predict(X_t).feature\n",
    "print(\"Prediction:\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96c2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 10.3 ms, total: 1.48 s\n",
      "Wall time: 1.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[106,   5],\n",
       "       [ 11,  66]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt_clf = DecisionTreeClassifier(metric='entropy', criterion={'partition_threshold': len(X_train) * 5e-2}).fit(X_train, y_train)\n",
    "dt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e4b990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst concave points (Branch=None)\n",
       "\tworst perimeter (Branch=0.1423499956727028)\n",
       "\t\t1 (Branch=105.95000076293945)\n",
       "\t\t1 (Branch=117.44999694824219)\n",
       "\t\t1 (Branch=87.36999893188477)\n",
       "\t\t0 (Branch=251.2)\n",
       "\t\t1 (Branch=101.64999771118164)\n",
       "\t0 (Branch=0.291)\n",
       "\tmean area (Branch=0.07441999763250351)\n",
       "\t\t1 (Branch=696.25)\n",
       "\t\t1 (Branch=576.7000122070312)\n",
       "\t\t1 (Branch=390.6000061035156)\n",
       "\t\t1 (Branch=2501.0)\n",
       "\t\t1 (Branch=440.8000030517578)\n",
       "\t\t1 (Branch=529.8000183105469)\n",
       "\t\t0 (Branch=883.25)\n",
       "\tmean radius (Branch=0.10954999923706055)\n",
       "\t\t0 (Branch=16.925000190734863)\n",
       "\t\t1 (Branch=13.704999923706055)\n",
       "\t\t1 (Branch=11.425000190734863)\n",
       "\t\t1 (Branch=15.045000076293945)\n",
       "\t\t1 (Branch=28.11)\n",
       "\t\t1 (Branch=12.329999923706055)\n",
       "\t\t1 (Branch=13.09499979019165)\n",
       "\t1 (Branch=0.08554999902844429)\n",
       "\tarea error (Branch=0.16029999405145645)\n",
       "\t\t0 (Branch=40.010000228881836)\n",
       "\t\t1 (Branch=17.02999973297119)\n",
       "\t\t0 (Branch=23.93000030517578)\n",
       "\t\t0 (Branch=50.954999923706055)\n",
       "\t\t0 (Branch=31.28499984741211)\n",
       "\t\t1 (Branch=22.125)\n",
       "\t\t0 (Branch=542.2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72776ddb",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "$var(t, \\mathcal{D})=\\frac{\\sum_{i=1}^n(t_i-\\bar{t})^2}{n-1}$\n",
    "\n",
    "$weighted\\ var(t, \\mathcal{D}) = \\sum_{l\\in levels(d)}{} \\frac{|\\mathcal{D}_{d=l}|}{|\\mathcal{D}|} \\times var(t, \\mathcal{D}_{d=l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4197bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(DecisionTreeEstimator):\n",
    "    \"\"\"A Rudimentary Decision Tree Regressor\"\"\"\n",
    "    def __init__(self, *, criterion={}):\n",
    "        super().__init__(criterion)\n",
    "\n",
    "    def variance(self, X, y):\n",
    "        if len(X) == 1:\n",
    "            return 0\n",
    "        return np.sum([(t-mean(y))**2 for t in y]) / (len(X)-1)\n",
    "\n",
    "    def weighted_variance(self, X, y, d):\n",
    "        weight = lambda t: len(X.loc[X[d]==t]) / len(X)\n",
    "        return np.sum([weight(t) * self.variance(X.loc[X[d]==t], y.loc[X[d]==t]) for t in X[d].unique()])\n",
    "\n",
    "    def make_tree(self, X, y, *, parent=None, branch=None, depth=0):\n",
    "        \"\"\"\n",
    "        Performs the ID3 algorithm\n",
    "\n",
    "        Base Cases\n",
    "        ----------\n",
    "        - all instances have the same target feature values\n",
    "        - dataset is empty, return a leaf node labeled with the majority class\n",
    "        - max_depth reached\n",
    "        - max number of instances in partitioned dataset reached\n",
    "        \"\"\"\n",
    "        make_node = lambda f, t: Node(feature=f, data=pd.concat([X, y], axis=1), branch=branch, parent=parent, depth=depth, leaf=t)\n",
    "\n",
    "        if len(y.unique()) == 1:\n",
    "            return make_node(y.iat[0], True)\n",
    "        elif X.empty:\n",
    "            return make_node(mean(y), True)\n",
    "        if self.criterion.get(\"max_depth\", float('inf')) <= depth:\n",
    "            return make_node(mean(y), True)\n",
    "        if self.criterion.get(\"partition_threshold\", float('-inf')) >= len(X):\n",
    "            return make_node(mean(y), True)\n",
    "\n",
    "        min_var = np.argmin([self.weighted_variance(X, y, d) for d in X.columns])\n",
    "\n",
    "        best_feature = X.columns[min_var]\n",
    "        best_node = deepcopy(make_node(best_feature, False))\n",
    "\n",
    "        X_levels = [self.partition(X, y, best_feature, level) for level in self.n_levels[best_feature]]\n",
    "\n",
    "        for *d, level in X_levels:\n",
    "            best_node.children.append(self.make_tree(*d, parent=best_node, branch=level, depth=depth+1))\n",
    "        return best_node\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = super().score(X, y)\n",
    "        return mean_squared_error(y, y_hat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee157954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Season': ['winter', 'winter', 'winter', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'autumn', 'autumn', 'autumn'],\n",
    "    'Work Day': ['false', 'false', 'true', 'false', 'true', 'true', 'false', 'true', 'true', 'false', 'false', 'true'],\n",
    "    'Rentals': [800, 826, 900, 2100, 4740, 4900, 3000, 5800, 6200, 2910, 2880, 2820],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "A, b = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2e6224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_regr = DecisionTreeRegressor().fit(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc9fedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Season (Branch=None)\n",
       "\tWork Day (Branch=winter)\n",
       "\t\t813 (Branch=false)\n",
       "\t\t900 (Branch=true)\n",
       "\tWork Day (Branch=spring)\n",
       "\t\t2100 (Branch=false)\n",
       "\t\t4820 (Branch=true)\n",
       "\tWork Day (Branch=summer)\n",
       "\t\t3000 (Branch=false)\n",
       "\t\t6000 (Branch=true)\n",
       "\tWork Day (Branch=autumn)\n",
       "\t\t2895 (Branch=false)\n",
       "\t\t2820 (Branch=true)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885a388",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "454ac107-8855-4862-8582-e1f55e2f6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=5, n_sample=0, eval=\"info_gain\", criterion={}):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_sample = n_sample\n",
    "        self.forest = [DecisionTreeClassifier(eval=eval, criterion=criterion) for _ in range(n_estimators)]\n",
    "\n",
    "    def sub_sample(self, X, n_sample=2):\n",
    "        \"\"\"Enforces feature randomness\"\"\"\n",
    "        return np.random.choice(X.columns.to_numpy(), n_sample, replace=False)\n",
    "\n",
    "    def bootstrap_sample(self, X, y, n_sample, key=True):\n",
    "        feature_subset = self.sub_sample(X, int(np.log2(len(X))))\n",
    "        d = pd.concat([X, y], axis=1)\n",
    "        d = d.sample(n=n_sample, replace=key)\n",
    "        return d.iloc[:, :-1][feature_subset], d.iloc[:, -1]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i, tree in enumerate(self.forest, start=1):\n",
    "            tree.fit(*self.bootstrap_sample(X, y, self.n_sample))\n",
    "            if debug:\n",
    "                print(f\"Decision Tree #{i} Complete\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        assert all(isinstance(model, DecisionTreeClassifier) for model in self.forest)\n",
    "        return mode([dt.predict(x).feature for dt in self.forest])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = [self.predict(X.iloc[x].to_frame().T) for x in range(len(X))]\n",
    "        return confusion_matrix(y, y_hat, labels=y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d17d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 13.7 ms, total: 1.9 s\n",
      "Wall time: 1.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[108,   3],\n",
       "       [ 12,  65]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForest(n_estimators=5, n_sample=len(X_train), criterion={'max_depth': 4}).fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f75164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
